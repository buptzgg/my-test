{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###导入相关的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import lxml\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###爬取上市公司的股票代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://hq.gucheng.com/gpdmylb.html')\n",
    "res_xpath = etree.HTML(res.text)\n",
    "node = res_xpath.xpath('//*[@id=\"stock_index_right\"]/div[3]/section/a[@href]/text()')\n",
    "codeall = re.findall(r'\\d{6}', str(node))\n",
    "code_list = []\n",
    "for node in codeall:\n",
    "    code_list.append(node)\n",
    "gp_list = code_list[3:1003]#股票代码\n",
    "# gp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###爬取1000家公司2010年到2020年的股票交易数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"20151118\"#股票交易的起始日期\n",
    "end_date = \"20201118\"#股票交易数据的终止日期\n",
    "for gp in gp_list:\n",
    "        download_url = \"http://quotes.money.163.com/service/chddata.html?code=1\"+gp+\"&start=\"+start_date+\"&end=\"+end_date+\"&fields=TCLOSE;HIGH;LOW;TOPEN;LCLOSE;CHG;PCHG;TURNOVER;VOTURNOVER;VATURNOVER;TCAP;MCAP\"        \n",
    "        #以6和3开头的股票代码code前都有0，以0开头的股票代码code前都有1，由于我爬取的股票都是0开头的，所以不考虑前一种情况\n",
    "        data = requests.get(download_url)\n",
    "        f = open(gp+'.csv','wb')     \n",
    "        for chunk in data.iter_content(chunk_size=10000):\n",
    "            if chunk:\n",
    "                f.write(chunk)#保存获取的股票交易数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###爬取1000家公司的公告新闻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for node in codeall:\n",
    "#     code_list.append(node)\n",
    "gp_list = code_list[616:1003]#获取对应公司的股票代码\n",
    "for dm in gp_list:\n",
    "    x=0\n",
    "    a0,a1,a2,a3,a4=[],[],[],[],[]\n",
    "    while True:\n",
    "        url = 'http://quotes.money.163.com/f10/gsgg_'+dm+',zjgg,'+str(x)+'.html'\n",
    "        x+=1\n",
    "        mes = requests.get(url)\n",
    "        selector = etree.HTML(mes.text)  \n",
    "        data = re.compile(r'暂无数据')\n",
    "        data1 = re.findall(data,mes.text)\n",
    "        if data1 == ['暂无数据']:\n",
    "            break\n",
    "        mes = requests.get(url)\n",
    "        selector = etree.HTML(mes.text)  \n",
    "        data = re.compile(r'<td class=\"align_c\">\\d{4}-\\d{2}-\\d{2}</td>')#正则匹配日期\n",
    "        data1 = re.findall(data,mes.text)\n",
    "        data_list=[]\n",
    "        for d in data1:\n",
    "            data = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n",
    "            data0 = re.findall(data,d)\n",
    "            data_list.append(data0)#获取公告内的日期\n",
    "        selector_xpath1 = selector.xpath('//tr/td/a/@href')#提取网页中有关公告的链接\n",
    "        selector_xpath2 = selector.xpath('//tr/td/a/@title')#提取网页中有关公告的文字\n",
    "        a=selector_xpath1[10:]#前十个链接都与公告无关，故不提取\n",
    "        b=selector_xpath2[9:] #获取公告名称\n",
    "        for i,j ,k in zip(a,b,data_list):\n",
    "            html='http://quotes.money.163.com/'+i#获取完整的公告链接\n",
    "            response = requests.get(html)\n",
    "            text = response.text\n",
    "            compil = re.compile(r'[\\u4e00-\\u9fa5。，？！“”：（）—%.、0-9]')#正则匹配公告中的中文\n",
    "            han = re.findall(compil,text)#正则匹配公告中的中文\n",
    "            a0.append(k[0])\n",
    "            a1.append(html)\n",
    "            a2.append(j)\n",
    "            var = \" \"\n",
    "            for f in han:\n",
    "                var+=f\n",
    "            a3.append(var)\n",
    "            a4.append(dm)\n",
    "    result = pd.DataFrame(zip(a0,a4,a1,a2,a3))\n",
    "    result.to_excel(f'{dm}公告.xlsx')#将公告内容保存在文件中"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
